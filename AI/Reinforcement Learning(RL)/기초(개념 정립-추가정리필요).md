## 1. RAG vs RL: 개념의 치환

RAG가 "외부 지식을 가져와 답변(Text)을 생성"하는 것이라면, RL은 **"환경의 상태를 읽어 보상을 최대화하는 행동(Action)을 결정"**하는 것입니다.

- **RAG의 Context** $\approx$ **RL의 Observation/State**: 현재 게임 화면이나 캐릭터의 데이터.
    
- **RAG의 Prompt** $\approx$ **RL의 Policy**: 입력(상태)을 보고 어떤 출력(행동)을 할지 결정하는 신경망 전략.
    
- **RAG의 Evaluation** $\approx$ **RL의 Reward**: 모델의 행동이 좋았는지(점수+) 나빴는지(사망-) 판단하는 기준.
    

---

## 2. RL의 "HuggingFace"와 "LangChain"은 무엇인가?

RL 분야에도 표준처럼 쓰이는 스택이 정해져 있습니다.

### ① Gymnasium (표준 인터페이스)

RAG에서 데이터 로더가 중요하듯, RL에서는 게임 환경과 통신하는 표준 인터페이스가 필요합니다. 과거 OpenAI Gym의 뒤를 잇는 **Gymnasium**이 표준입니다.

- `env.step(action)`: 행동을 보내고 다음 상태와 보상을 받음.
    
- `env.reset()`: 게임 초기화.
    

### ② Stable Baselines3 (SB3 / "RL의 LangChain")

가장 대중적인 RL 알고리즘 라이브러리입니다. PPO, DQN, SAC 같은 검증된 알고리즘이 구현되어 있어, RAG에서 Chain을 구성하듯 몇 줄의 코드로 학습을 시작할 수 있습니다.

- PyTorch 기반이며, 인터페이스가 Scikit-learn처럼 직관적입니다.
    

### ③ CleanRL (코드 커스터마이징용)

SB3가 추상화되어 있다면, **CleanRL**은 알고리즘 하나를 파일 하나(Single-file)에 다 때려 넣은 라이브러리입니다. 내부 로직을 직접 수정해야 하는 개발자들에게 인기가 많습니다.



**Hello World:** `Gymnasium`에서 제공하는 'CartPole'이나 'LunarLander' 환경을 `Stable Baselines3`로 10분 만에 학습시켜 보세요. (학습이 진행되며 보상 그래프가 올라가는 것을 보는 게 첫걸음입니다.)


- **Fast Capture:** `MSS` 라이브러리를 사용하여 초당 30회 이상 게임 화면을 캡처합니다.
    
- **Preprocessing:** 캡처한 고화질 화면을 AI가 처리하기 쉽게 **흑백화(Grayscale)** 하고 **크기를 축소(예: 84x84)** 합니다.
    
- **Frame Stacking:** 정지 화면만으로는 적의 이동 속도를 알 수 없으므로, 최근 4장의 화면을 겹쳐서(Stacking) AI에게 넘겨줍니다. (이게 RL의 표준 기법입니다.)
    
- **Action:** AI가 "오른쪽 화살표"라고 판단하면 `pynput`이나 `pywin32`로 윈도우에 키 입력을 보냅니다.

https://novaraz.tistory.com/30