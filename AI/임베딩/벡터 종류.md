## Dense Retrieval (밀집 벡터)
일반적으로 사용되는 LLM 임베딩 방식
벡터의 각도만을 상정하여 텍스트의 의미적 유사성을 포착한다.

따라서 밥먹고싶다와 배고프다라는 전혀 다른 두 단어가 밀접한 연관성을 갖는다.
하지만 `error: 0xACP329SXIWQ`에 대해서 ==오류가 난 상황==이라는 추상적인 벡터값을 가지게 되기 때문에 "error: 0xACP329SXIWQ (화면 에러)가 난 상황"이라는 구체적인 정보를 잃게 된다.
### 원리
이 벡터를 탐구하다보면 1024차원, 768차원, 2048차원등의 단어를 접할 수 있는데 해당 문장을 설명할 수 있는 특징점의 갯수라고 생각하면 된다.

Dense Vector 형식을 보면 차원 갯수만큼의 요소가 배열에 들어온다.
`[0.52553, -0.1284, 0.18324.... 총 1024개]`

1차원에선 표현할 수 있는 특징의 갯수가 하나다.
`1차원=[생물:1]`: 생물임
`1차원=[생물:-1]`: 무생물임
`1차원=[생물:0]`: 애매함(바이러스)

2차원이 되면 2개의 축을 사용하게 되면서 표현할 수 있는 것들이 늘어난다.

`2차원=[생물:1, 이동성:1]`: 강아지, 사람, 기타 등등
`2차원=[생물:1, 이동성:-1]`: 버섯, 풀, 나무

이러한 특징들이 1024개 나열되면 무수히 많은 개체들을 표현할 수 있으며 비슷한 특성을 가진 개체들을 같은 공간 상에 클러스터링할 수 있다.

## Sparse Retrieval (희소 벡터)
단어 각각에 대한 가중치가 부여된다. 따라서 밥 먹고싶다와 배가 고프다는 각각 `밥, 먹고, 싶다(또는 먹고싶다)`와 `배, 고프다` 와 같이 토큰화되어 각각의 키워드가 가중치를 갖는다. 따라서 밀집 벡터에 비해 배고프다, 밥먹고싶다와 같이 단어 자체의 형태가 다르면 유사성이 떨어지는 단어로 본다. 그러나 배가 너무 고파서 힘이들어와 배는 많이 고픈데 밥 안 먹고 싶어와 같이 중요한 키워드가 일치할 경우 유사성을 높게 친다.

실제 코드를 돌려본 결과를 아래 적는다.

```python
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True) 

sentences = ["Hello World"]

output = model.encode(
        sentences,
        batch_size=12,
        max_length=8192,
        return_sparse=True, # 희소 벡터
        return_colbert_vecs=True 
    )
embeddings = output['lexical_weights']
```

이때 lexical_weights 값을 출력한 결과를 보자.

```
'35378': 0.278
'6661': 0.283
```

Hello가 35378, World가 6661로 임베딩된 것을 볼 수 있다.

이 데이터를 보면 알 수 있겠지만 배가 고프다와 밥 먹고 싶다 사이에는 어떤 연관성도 찾을 수 없는 것을 알 수 있다. 하지만 반대로 `error: 0xACP329SXIWQ`라는 텍스트를 임베딩하면 `error: 0xACP329SXIWQ`로 검색했을 때 키값이 100% 일치하게 되므로 해당 상황의 특수성을 밀집 벡터에 비해 잘 보존할 수 있다.

참고로 여기서 35378이나 6661은 벡터값이 아니라 토큰의 ID라고 보면 된다.
A가 아스키코드로 65인것처럼 Hello 자체가 35378이라는 토큰 ID를 갖는다.

## Multi-vector Retrieval (ColBERT)
문장의 모든 토큰마다 개별 벡터를 생성한다.
따라서 임베딩 할 때 사용하는 목적이 아니라 밀집벡터+희소벡터로 하이브리드 검색을 한 후 TOP K에 대해 Re-Ranking을 진행할 때 ColBERT 벡터를 활용해서 정확도 높은 문서가 상위에 랭크되게 하는 목적으로 사용한다.
이 값 자체를 임베딩하는 건 매우 비효율적이므로 반환받은 문서들에 한해서 해당 연산을 통해 Re-Ranking을 진행하면 좋다.