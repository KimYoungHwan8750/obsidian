로컬 LLM 개발을 하려고 모델을 알아보니 Qwen이 생태계 교란중이라고 한다.
중국에서 만들었기 때문에 한자 데이터를 많이 학습했고 한국어 LLM 개발할 때 성능이 우수하다.

0.5B, 1.5B, 3B등 토큰을 상세하게 나눠 모델을 출시해서 목적에 맞게 사용하기 좋다.

## 0.5B 너무 작은 거 아니야?
그런 생각이 들수도 있다. 하지만 토큰 수는 AI 성능과 완전 정비례하지 않는다. 예를 들면 72B 같은 경우 당연히 0.5B가 학습한 내용을 모두 포함하되, 다소 퀄리티가 좋지 않은 데이터도 모두 학습시킨 것이다. 0.5B는 엄선하고 엄선하여 정제된 데이터만을 학습시켰고 72B는 사용자가 고성능 컴퓨터를 사용한다는 전제하에 남은 리소스를 활용하기 위해 다소 정제되지 않은 데이터라도 학습시켜서 AI의 범용성을 증가시킨 것이라고 보면 된다,

**0.5B / 1.5B / 3B:** 아주 가벼움. 라즈베리파이나 핸드폰에서도 돌아감
**7B / 14B:** 일반 게이밍 PC(그래픽카드 RTX 3060~4060 수준)에서 잘 돌아감. 성능은 과거 GPT-3.5보다 좋은 수준.
**32B / 72B:** 기업용 서버나 고사양 PC 필요. 성능은 GPT-4급.

문서 파싱: 